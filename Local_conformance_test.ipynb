{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, SimpleRNN, LSTM, Dropout, Bidirectional\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = ['A', 'B', 'D', 'E', 'I']\n",
    "trace2 = ['A', 'C', 'D', 'G', 'H', 'F', 'I']\n",
    "trace3 = ['A', 'C', 'G', 'D', 'H', 'F', 'I']\n",
    "trace4 = ['A', 'C', 'H', 'D', 'F', 'I']\n",
    "trace5 = ['A', 'C', 'D', 'H', 'F', 'I']\n",
    "\n",
    "\n",
    "traces = [trace1, trace2, trace3, trace4, trace5]\n",
    "frequencies = [1207, 145, 56, 23, 28]\n",
    "\n",
    "GT_log = []\n",
    "for f in range(0, len(frequencies)):\n",
    "    print(traces[f], frequencies[f])\n",
    "    for i in range(0, frequencies[f]):\n",
    "        GT_log.append(traces[f])\n",
    "        \n",
    "random.shuffle(GT_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(trace, log):\n",
    "    if trace in log:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PARTget_voc(log):\n",
    "    outputset1 = set([])\n",
    "    #outputset2 = set([]) #for voc with only log2 for creating antilog\n",
    "    for trace in log:\n",
    "        for act in trace:\n",
    "            outputset1.add(act)\n",
    "    voc = list(outputset1)\n",
    "    return voc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PARTadd_noise_name(logdummy, voc, percentage):\n",
    "    changelog = []\n",
    "    amount_of_noisy_traces = int(percentage * len(logdummy))\n",
    "    noisy_examples = random.sample(range(0, len(logdummy)), amount_of_noisy_traces)\n",
    "    for i in range(0, len(logdummy)):\n",
    "        trace = copy.deepcopy(logdummy[i])\n",
    "        if i in noisy_examples:\n",
    "            random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "            new = random.choice(voc)\n",
    "            #print(new)\n",
    "            trace[random_act] = new\n",
    "            done = check(trace, logdummy)\n",
    "            while done == True:\n",
    "                #print(\"double trouble\")\n",
    "                random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "                new = random.choice(voc)\n",
    "                #print(new)\n",
    "                trace[random_act] = new\n",
    "                done = check(trace, logdummy)\n",
    "            changelog.append(trace)\n",
    "        else:\n",
    "            changelog.append(trace)\n",
    "    return changelog\n",
    "\n",
    "def PARTadd_noise_order(logdummy, percentage):\n",
    "    changelog = []\n",
    "    amount_of_noisy_traces = int(percentage * len(logdummy))\n",
    "    noisy_examples = random.sample(range(0, len(logdummy)), amount_of_noisy_traces)\n",
    "    for i in range(0, len(logdummy)):\n",
    "        trace = copy.deepcopy(logdummy[i])\n",
    "        if i in noisy_examples:\n",
    "            random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "            random_act2 = random.randint(0, len(logdummy[i]) - 1)\n",
    "            first = copy.copy(logdummy[i][random_act])\n",
    "            second = copy.copy(logdummy[i][random_act2])\n",
    "            #print(first, second)\n",
    "            trace[random_act] = second\n",
    "            trace[random_act2] = first\n",
    "            done = check(trace, logdummy)\n",
    "            while done == True:\n",
    "                random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "                random_act2 = random.randint(0, len(logdummy[i]) - 1)\n",
    "                first = copy.copy(logdummy[i][random_act])\n",
    "                second = copy.copy(logdummy[i][random_act2])\n",
    "                #print(first, second)\n",
    "                trace[random_act] = second\n",
    "                trace[random_act2] = first\n",
    "                done = check(trace, logdummy)\n",
    "            changelog.append(trace)\n",
    "        else:\n",
    "            changelog.append(trace)\n",
    "    return changelog\n",
    "\n",
    "\n",
    "\n",
    "def PARTadd_noise_delete(logdummy, percentage):\n",
    "    changelog = []\n",
    "    amount_of_noisy_traces = int(percentage * len(logdummy))\n",
    "    noisy_examples = random.sample(range(0, len(logdummy)), amount_of_noisy_traces)\n",
    "    for i in range(0, len(logdummy)):\n",
    "        trace = copy.deepcopy(logdummy[i])\n",
    "        if i in noisy_examples:\n",
    "            random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "            del trace[random_act]\n",
    "            changelog.append(trace)\n",
    "        else:\n",
    "            changelog.append(trace)\n",
    "    return changelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PARTadd_noise(logdummy, percentage):\n",
    "    l = copy.deepcopy(logdummy)\n",
    "    voc = PARTget_voc(logdummy)\n",
    "    l1 = PARTadd_noise_name(l, voc, percentage)\n",
    "    #l2 = PARTadd_noise_order(l, percentage)\n",
    "    #l3 = PARTadd_noise_delete(l2, percentage)\n",
    "    return(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGetter(deque):\n",
    "    def appendleft(self, line):\n",
    "        print(line)\n",
    "        super().appendleft(line)\n",
    "        \n",
    "def remove_stuff(log): #removes short traces\n",
    "    #new_log = [i for i in log if len(i) > 5]\n",
    "    new_log = [i for i in log if len(i) > 1]\n",
    "    return new_log\n",
    "    \n",
    "        \n",
    "def remove_plus(dummylog):\n",
    "    newlog = copy.deepcopy(dummylog)\n",
    "    newlog = [[item.replace('+', '') for item in lst] for lst in newlog]\n",
    "    return newlog\n",
    "       \n",
    "def fix_duplicates(dummylog):\n",
    "    newlog = []\n",
    "    for trace in dummylog:\n",
    "        newtrace = trace[::2]\n",
    "        newlog.append(newtrace)\n",
    "    return newlog\n",
    "\n",
    "def check_duplicates(dummylog): #not a good test but just to be sure\n",
    "    if len(dummylog) > 1 and dummylog[0][0] == dummylog[0][1] and dummylog[1][0] == dummylog[1][1]:\n",
    "        return True\n",
    "    elif len(dummylog) == 1 and dummylog[0][0] == dummylog[0][1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_log(modelfilename, size):\n",
    "    gateway = JavaGateway.launch_gateway(classpath=\"./jars/*\", \n",
    "                                         redirect_stdout=OutputGetter())\n",
    "    petri_and_marking = gateway.jvm.org.processmining.plugins.kutoolbox.utils.ImportUtils.openPetrinet(\n",
    "        gateway.jvm.java.io.File(modelfilename)\n",
    "    )\n",
    "    if not petri_and_marking[1].size():\n",
    "        print(\"Creating initial marking\")\n",
    "        petri_and_marking[1] = gateway.jvm.org.processmining.plugins.kutoolbox.utils.PetrinetUtils. \\\n",
    "            getInitialMarking(petri_and_marking[0])\n",
    "\n",
    "    petri_and_marking[1].size()\n",
    "    settings = gateway.jvm.org.processmining.plugins.loggenerator.utils.GeneratorSettings()\n",
    "    simulator = gateway.jvm.org.processmining.plugins.loggenerator.utils.GeneratorSettings.SimulationAlgorithm\n",
    "\n",
    "    # See https://github.com/Macuyiko/processmining-prom/blob/master/loggenerator/\n",
    "    #             org/processmining/plugins/loggenerator/utils/GeneratorSettings.java\n",
    "\n",
    "    for t in petri_and_marking[0].getTransitions().iterator():\n",
    "        isInvisible = t.getLabel() == \"\" or t.isInvisible()\n",
    "        label = t.getLabel()\n",
    "        isTextInvisible = label.startswith(\"inv_\") or \"$\" in label\n",
    "        mapped = label.replace(\"+complete\", \"\").replace(\"\\\\ncomplete\", \"\").replace(\"\\\\n\", \"\") \\\n",
    "            if not isInvisible and not isTextInvisible else \"\"\n",
    "        arr = gateway.new_array(gateway.jvm.int, 4)\n",
    "        arr[0], arr[1], arr[2], arr[3] = 60, 0, 60, 0\n",
    "        settings.getTransitionNames().put(t, mapped)\n",
    "        settings.getTransitionWeights().put(t, 10)\n",
    "        settings.getTransitionTimings().put(t, arr)\n",
    "        #print(t.getId(), ':', label, '--->', mapped)\n",
    "\n",
    "        settings.setNrTraces(size)\n",
    "        settings.setRandomMinInGroup(1)\n",
    "        settings.setRandomMaxInGroup(1)\n",
    "        settings.setMustReachEnd(True)\n",
    "        settings.setMustConsumeAll(False)\n",
    "        settings.setMaxTimesMarkingSeen(7)\n",
    "\n",
    "        settings.setSimulationMethod(simulator.Random) # Random, Complete, or Distinct\n",
    "\n",
    "        # settings.setRestartAfter(1000)\n",
    "        # settings.setSkipChance(0.85)\n",
    "        # settings.setAlsoConsiderPartial(False)\n",
    "    xlog = gateway.jvm.org.processmining.plugins.loggenerator.PetriNetLogGenerator.generate(\n",
    "        petri_and_marking[0], \n",
    "        petri_and_marking[1], \n",
    "        settings, \n",
    "        #None # or for output: gateway.jvm.org.processmining.plugins.kutoolbox.eventlisteners.PluginEventListenerCLI()\n",
    "        gateway.jvm.org.processmining.plugins.kutoolbox.eventlisteners.PluginEventListenerCLI()\n",
    "    )\n",
    "\n",
    "    temp_log = []\n",
    "    for trace in xlog:\n",
    "        #print([gateway.jvm.org.deckfour.xes.extension.std.XConceptExtension.instance().extractName(e) for e in trace])\n",
    "        temp_log.append([gateway.jvm.org.deckfour.xes.extension.std.XConceptExtension.instance().extractName(e) for e in trace])\n",
    "\n",
    "    model_log = remove_stuff(temp_log)\n",
    "\n",
    "\n",
    "    if check_duplicates(model_log) == True:\n",
    "        print('Fixing duplicates')\n",
    "        model_log = fix_duplicates(model_log)\n",
    "    return(model_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_log = get_model_log(\"Model1.pnml\", 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc(log1, log2):\n",
    "    log = log1 + log2\n",
    "    outputset1 = set([])\n",
    "    #outputset2 = set([]) #for voc with only log2 for creating antilog\n",
    "    for trace in log:\n",
    "        for act in trace:\n",
    "            outputset1.add(act)\n",
    "    voc = list(outputset1)\n",
    "    return voc\n",
    "    \n",
    "\n",
    "\n",
    "def add_noise_name(logdummy, voc):\n",
    "    changelog = []\n",
    "    for i in range(0, len(logdummy)):\n",
    "        trace = copy.deepcopy(logdummy[i])\n",
    "        random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "        new = random.choice(voc)\n",
    "        #print(new)\n",
    "        trace[random_act] = new\n",
    "        changelog.append(trace)\n",
    "    return changelog\n",
    "\n",
    "def add_noise_order(logdummy):\n",
    "    changelog = []\n",
    "    for i in range(0, len(logdummy)):\n",
    "        trace = copy.deepcopy(logdummy[i])\n",
    "        random_act = random.randint(0, len(logdummy[i]) - 1)\n",
    "        random_act2 = random.randint(0, len(logdummy[i]) - 1)\n",
    "        first = copy.copy(logdummy[i][random_act])\n",
    "        second = copy.copy(logdummy[i][random_act2])\n",
    "        #print(first, second)\n",
    "        trace[random_act] = second\n",
    "        trace[random_act2] = first\n",
    "        changelog.append(trace)\n",
    "    return changelog\n",
    "    \n",
    "\n",
    "#SHOULD CHECK IF WE WANT TO DO THIS!!!!!!\n",
    "def delete_not_new(log, antilog):\n",
    "    unique = []\n",
    "    for trace in log:\n",
    "        if trace not in unique:\n",
    "            unique.append(trace)\n",
    "        else:\n",
    "            continue\n",
    "    #print(unique)\n",
    "    new_antilog = []\n",
    "    for trace in antilog:\n",
    "        if trace not in unique:\n",
    "            new_antilog.append(trace)\n",
    "        else:\n",
    "            continue\n",
    "    return(new_antilog)\n",
    "\n",
    "\n",
    "def get_antilog_noise(logdummy, voc):\n",
    "    l = copy.deepcopy(logdummy)\n",
    "    l1 = add_noise_name(l, voc)\n",
    "    l2 = add_noise_order(l1)\n",
    "    antilog = delete_not_new(logdummy, l2)\n",
    "    return(antilog)\n",
    "    \n",
    "def get_antilog_random(log, voc):\n",
    "    min_length = len(min(log, key=len))\n",
    "    max_length = len(max(log, key=len)) #now we just take random between min and max, should probably be changed to normal distribution\n",
    "    length_antilog = len(log) #for size antilog = size log, but can be altered\n",
    "    antilog = []\n",
    "    for i in range(0, length_antilog):\n",
    "        size = random.randint(min_length,max_length)\n",
    "        antitrace = []\n",
    "        for j in range(0, size):\n",
    "            index = random.randint(0,len(voc)-1)\n",
    "            antitrace.append(voc[index])\n",
    "        antilog.append(antitrace)\n",
    "    return(antilog)\n",
    "\n",
    "def padding(dumlog, max_length, pad):\n",
    "    log = copy.deepcopy(dumlog)\n",
    "    newlog =  []\n",
    "    for trace in log:\n",
    "        if len(trace)>max_length:\n",
    "            while len(trace)>max_length:\n",
    "                trace.pop()\n",
    "        elif len(trace)<max_length:   \n",
    "            while len(trace)<max_length:\n",
    "                trace.append(pad)\n",
    "        newlog.append(trace)\n",
    "    return newlog\n",
    "\n",
    "def get_batches(log): #get same sized batches in list of numpy multidim arrays\n",
    "    #log is log WITH label!\n",
    "    min_length = len(min([item[0] for item in log], key=len))\n",
    "    max_length = len(max([item[0] for item in log], key=len))\n",
    "    print(min_length, max_length)\n",
    "    Xbatches = []\n",
    "    Ybatches = []\n",
    "    for size in range(min_length, max_length+1):\n",
    "        print(size)\n",
    "        listX = []\n",
    "        listY = []\n",
    "        for i in range(0, len(log)):\n",
    "            if len(log[i][0]) == size:\n",
    "                listX.append(log[i][0])\n",
    "                listY.append(log[i][1])\n",
    "        if len(listY) == 0:\n",
    "            print(size, \"No traces\")\n",
    "            continue\n",
    "        arrayX = np.array(listX)\n",
    "        #arrayX = np.expand_dims(arrayX, axis=2)\n",
    "        #print(arrayX)\n",
    "        arrayY = np.array(listY)\n",
    "        Xbatches.append(arrayX)\n",
    "        Ybatches.append(arrayY)   \n",
    "    return(Xbatches, Ybatches)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dumlog2, dumantilog, voc):\n",
    "\n",
    "    maximumlength = len(max(dumlog2, key=len))\n",
    "\n",
    "    #dumlog2 = padding(dumlog2, maximumlength, \"Z\")\n",
    "    #dumantilog = padding(dumantilog, maximumlength, \"Z\")\n",
    "\n",
    "    log2 = copy.deepcopy(dumlog2)\n",
    "    antilog = copy.deepcopy(dumantilog)\n",
    "    #voc.append(\"Z\")\n",
    "    label_encoder = LabelEncoder() #label encoder \n",
    "    label_encoder.fit(voc)\n",
    "    \n",
    "    #list of arrays (traces) with encoded activities as numpy array\n",
    "    trainlog = []\n",
    "    for i in range(len(log2)):\n",
    "        dummytrace = label_encoder.transform(log2[i])\n",
    "        trace = dummytrace.tolist()\n",
    "        trainlog.append([trace,1]) #1 = label true trace\n",
    "    for i in range(len(antilog)):\n",
    "        dummytrace = label_encoder.transform(antilog[i])\n",
    "        trace = dummytrace.tolist()\n",
    "        trainlog.append([trace,0]) #0 = label antitrace\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for trace in trainlog:\n",
    "        X.append(trace[0])\n",
    "        Y.append(trace[1])\n",
    "    X = sequence.pad_sequences(X, maxlen=maximumlength)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    print(\"Fixed preprocessing\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(voc),output_dim=4, input_length=maximumlength))\n",
    "    #model.add(LSTM(units=32))\n",
    "    model.add(Bidirectional(SimpleRNN(units=8)))\n",
    "    #model.add(Bidirectional(LSTM(units=8)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "    hist = model.fit(X, Y, batch_size=64, epochs=40, verbose = 0, validation_split = 0.2)\n",
    "    print(\"training done\")\n",
    "    return(model, maximumlength, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit(model, dumlog1, maximumlength, label_encoder):\n",
    "    #dumlog1 = padding(dumlog1, maximumlength, \"Z\")\n",
    "    log1 = copy.deepcopy(dumlog1)\n",
    "    testlog = []\n",
    "    for trace in log1:\n",
    "        dummytrace = label_encoder.transform(trace)\n",
    "        testlog.append(dummytrace)\n",
    "    testlog = sequence.pad_sequences(testlog, maxlen=maximumlength)\n",
    "    testlog = np.array(testlog)\n",
    "    squeezed = model.predict(testlog)\n",
    "    fit1 = np.average(squeezed)\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    for pred in squeezed:\n",
    "        if pred < 0.5:\n",
    "            count_0 = count_0 + 1\n",
    "        if pred >= 0.5:\n",
    "            count_1 = count_1 + 1\n",
    "    #print(\"Amount predicted true: \", count_0, \". Amount predicted antitrace: \", count_1)\n",
    "    fitness = count_1/(count_1 + count_0)\n",
    "    return(fit1, fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we are training the discriminator on the model log ONCE.\n",
    "We then add different levels of noise to the GT log and put that trough the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fitness(filename, modelsize, GT_log, noises): \n",
    "    model_log = get_model_log(filename, modelsize)\n",
    "    voc = get_voc(GT_log, model_log)\n",
    "    antilog = get_antilog_random(model_log, voc)\n",
    "    \n",
    "    model, maxlength, enc = get_model(model_log, antilog, voc)\n",
    "    fit1list = []\n",
    "    fit2list = []\n",
    "    for n in noises:\n",
    "        noisy_log = PARTadd_noise(GT_log, n)\n",
    "        print(\"NOISE\", n)\n",
    "        fit1, fit2 = get_fit(model, noisy_log, maxlength, enc)\n",
    "        print(\"PROB: \", fit1, \" COUNT: \", fit2)\n",
    "        fit1list.append(fit1)\n",
    "        fit2list.append(fit2)\n",
    "    return(fit1list, fit2list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we are training the discriminator on the GT log ONCE.\n",
    "We then add different levels of noise to the model log and put that trough the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_precision(filename, modelsize, GT_log, noises): \n",
    "    model_log = get_model_log(filename, modelsize)\n",
    "    voc = get_voc(model_log, GT_log)\n",
    "    antilog = get_antilog_random(GT_log, voc)\n",
    "    \n",
    "    model, maxlength, enc = get_model(GT_log, antilog, voc)\n",
    "    fit1list = []\n",
    "    fit2list = []\n",
    "    for n in noises:\n",
    "        noisy_log = PARTadd_noise(model_log, n)\n",
    "        print(\"NOISE\", n)\n",
    "        fit1, fit2 = get_fit(model, noisy_log, maxlength, enc)\n",
    "        print(\"PROB: \", fit1, \" COUNT: \", fit2)\n",
    "        fit1list.append(fit1)\n",
    "        fit2list.append(fit2)\n",
    "    return(fit1list, fit2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitlist1, fitlist2 = test_fitness(\"Model1.pnml\", 1500, GT_log, noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preclist1, preclist2 = test_precision(\"Model1.pnml\", 1500, GT_log, noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GT_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(noises, fitlist1, label=\"Recall prob.\")\n",
    "plt.plot(noises, fitlist2, label=\"Recall count\")\n",
    "plt.plot(noises, preclist1, label=\"Precision prob.\")\n",
    "plt.plot(noises, preclist2, label=\"Precision count\")\n",
    "plt.xlabel('Fraction of instances with noise')\n",
    "plt.ylabel('Recall/Precision')\n",
    "plt.title('Recall/Precision doing random swaps')\n",
    "plt.legend()\n",
    "plt.savefig(\"swap.pdf\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(noises, fitlist1, label=\"Recall prob.\")\n",
    "plt.plot(noises, fitlist2, label=\"Recall count\")\n",
    "plt.plot(noises, preclist1, label=\"Precision prob.\")\n",
    "plt.plot(noises, preclist2, label=\"Precision count\")\n",
    "plt.xlabel('Fraction of instances with noise')\n",
    "plt.ylabel('Recall/Precision')\n",
    "plt.title('Recall/Precision doing random replacements')\n",
    "plt.legend()\n",
    "plt.savefig(\"replacement.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GT_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_log = get_model_log(\"Model1.pnml\", 1500)\n",
    "voc = get_voc(GT_log, GT_log)\n",
    "antilog = get_antilog_random(GT_log, voc)\n",
    "#antilog = get_antilog_noise(GT_log, voc)\n",
    "model, maxlength, enc = get_model(GT_log, antilog, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = [['A', 'B', 'D', 'E', 'I'], ['A', 'B', 'D', 'E', 'I'], ['A', 'B', 'D', 'E', 'I']]\n",
    "\n",
    "fitting2 = [['A', 'C', 'H', 'D', 'F', 'I'], ['A', 'C', 'H', 'D', 'F', 'I'], ['A', 'C', 'H', 'D', 'F', 'I']]\n",
    "\n",
    "nonfitting = [['A', 'B', 'E', 'D', 'I'], ['A', 'B', 'E', 'D', 'I'], ['A', 'B', 'E', 'D', 'I']]\n",
    "\n",
    "alsononfitting =  [['A', 'D', 'B', 'E', 'I'], ['A', 'D', 'B', 'E', 'I'], ['A', 'D', 'B', 'E', 'I']]\n",
    "\n",
    "nonfitting3 =  [['A', 'B', 'E', 'I'], ['A', 'B', 'E', 'I'], ['A', 'B', 'E', 'I']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1, bla1 = get_fit(model, fitting, maxlength, enc)\n",
    "\n",
    "fit2, bla2 = get_fit(model,fitting2,maxlength,enc)\n",
    "\n",
    "nonfit1, nonfit2 = get_fit(model, nonfitting, maxlength, enc)\n",
    "\n",
    "alsononfit1, alsononfit2 = get_fit(model, alsononfitting, maxlength, enc)\n",
    "\n",
    "nonfit1three, nonfit2three = get_fit(model, nonfitting3, maxlength, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit1, bla1)\n",
    "print(fit2, bla2)\n",
    "print(nonfit1, nonfit2)\n",
    "print(alsononfit1, alsononfit2)\n",
    "print(nonfit1three, nonfit2three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = ['A', 'B', 'D', 'E', 'I']\n",
    "trace2 = ['A', 'C', 'D', 'G', 'H', 'F', 'I']\n",
    "trace3 = ['A', 'C', 'G', 'D', 'H', 'F', 'I']\n",
    "trace4 = ['A', 'C', 'H', 'D', 'F', 'I']\n",
    "trace5 = ['A', 'C', 'D', 'H', 'F', 'I']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit_distribution(model, dumlog1, maximumlength, label_encoder):\n",
    "    #dumlog1 = padding(dumlog1, maximumlength, \"Z\")\n",
    "    log1 = copy.deepcopy(dumlog1)\n",
    "    testlog = []\n",
    "    for trace in log1:\n",
    "        dummytrace = label_encoder.transform(trace)\n",
    "        testlog.append(dummytrace)\n",
    "    testlog = sequence.pad_sequences(testlog, maxlen=maximumlength)\n",
    "    testlog = np.array(testlog)\n",
    "    squeezed = model.predict(testlog)\n",
    "    fit1 = np.average(squeezed)\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    for pred in squeezed:\n",
    "        if pred < 0.5:\n",
    "            count_0 = count_0 + 1\n",
    "        if pred >= 0.5:\n",
    "            count_1 = count_1 + 1\n",
    "    #print(\"Amount predicted true: \", count_0, \". Amount predicted antitrace: \", count_1)\n",
    "    fitness = count_1/(count_1 + count_0)\n",
    "    return(squeezed, fit1, fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log = get_model_log(\"Model1.pnml\", 1500)\n",
    "voc = get_voc(model_log, GT_log)\n",
    "#antilog = get_antilog_random(GT_log, voc)\n",
    "antilog = get_antilog_noise(model_log, voc)\n",
    "model, maxlength, enc = get_model(model_log, antilog, voc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noisy_log = PARTadd_noise(GT_log, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, fitprob, fitcount = get_fit_distribution(model, noisy_log, maxlength, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib histogram\n",
    "plt.hist(dist, color = 'blue', edgecolor = 'black',\n",
    "         bins = 40)\n",
    "\n",
    "# seaborn histogram\n",
    "sns.distplot(dist, hist=True, kde=False, \n",
    "             bins=40, color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Distribution Recall values of different instances')\n",
    "plt.xlabel('Probability trace')\n",
    "plt.ylabel('#instances')\n",
    "plt.savefig(\"Distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitprob, fitcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
