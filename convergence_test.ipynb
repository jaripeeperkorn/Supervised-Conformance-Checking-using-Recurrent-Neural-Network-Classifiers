{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from collections import deque\n",
    "\n",
    "import pad_discriminator_conformance\n",
    "\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = ['A', 'B', 'D', 'E', 'I']\n",
    "trace2 = ['A', 'C', 'D', 'G', 'H', 'F', 'I']\n",
    "trace3 = ['A', 'C', 'G', 'D', 'H', 'F', 'I']\n",
    "trace4 = ['A', 'C', 'H', 'D', 'F', 'I']\n",
    "trace5 = ['A', 'C', 'D', 'H', 'F', 'I']\n",
    "\n",
    "\n",
    "traces = [trace1, trace2, trace3, trace4, trace5]\n",
    "frequencies = [1207, 145, 56, 23, 28]\n",
    "\n",
    "GT_log = []\n",
    "for f in range(0, len(frequencies)):\n",
    "    print(traces[f], frequencies[f])\n",
    "    for i in range(0, frequencies[f]):\n",
    "        GT_log.append(traces[f])\n",
    "        \n",
    "random.shuffle(GT_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGetter(deque):\n",
    "    def appendleft(self, line):\n",
    "        print(line)\n",
    "        super().appendleft(line)\n",
    "        \n",
    "def remove_stuff(log): #removes short traces\n",
    "    #new_log = [i for i in log if len(i) > 5]\n",
    "    new_log = [i for i in log if len(i) > 1]\n",
    "    return new_log\n",
    "    \n",
    "        \n",
    "def remove_plus(dummylog):\n",
    "    newlog = copy.deepcopy(dummylog)\n",
    "    newlog = [[item.replace('+', '') for item in lst] for lst in newlog]\n",
    "    return newlog\n",
    "       \n",
    "def fix_duplicates(dummylog):\n",
    "    newlog = []\n",
    "    for trace in dummylog:\n",
    "        newtrace = trace[::2]\n",
    "        newlog.append(newtrace)\n",
    "    return newlog\n",
    "\n",
    "def check_duplicates(dummylog): #not a good test but just to be sure\n",
    "    if len(dummylog) > 1 and dummylog[0][0] == dummylog[0][1] and dummylog[1][0] == dummylog[1][1]:\n",
    "        return True\n",
    "    elif len(dummylog) == 1 and dummylog[0][0] == dummylog[0][1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_log(modelfilename, size):\n",
    "    gateway = JavaGateway.launch_gateway(classpath=\"./jars/*\", \n",
    "                                         redirect_stdout=OutputGetter())\n",
    "    petri_and_marking = gateway.jvm.org.processmining.plugins.kutoolbox.utils.ImportUtils.openPetrinet(\n",
    "        gateway.jvm.java.io.File(modelfilename)\n",
    "    )\n",
    "    if not petri_and_marking[1].size():\n",
    "        #print(\"Creating initial marking\")\n",
    "        petri_and_marking[1] = gateway.jvm.org.processmining.plugins.kutoolbox.utils.PetrinetUtils. \\\n",
    "            getInitialMarking(petri_and_marking[0])\n",
    "\n",
    "    petri_and_marking[1].size()\n",
    "    settings = gateway.jvm.org.processmining.plugins.loggenerator.utils.GeneratorSettings()\n",
    "    simulator = gateway.jvm.org.processmining.plugins.loggenerator.utils.GeneratorSettings.SimulationAlgorithm\n",
    "\n",
    "    # See https://github.com/Macuyiko/processmining-prom/blob/master/loggenerator/\n",
    "    #             org/processmining/plugins/loggenerator/utils/GeneratorSettings.java\n",
    "\n",
    "    for t in petri_and_marking[0].getTransitions().iterator():\n",
    "        isInvisible = t.getLabel() == \"\" or t.isInvisible()\n",
    "        label = t.getLabel()\n",
    "        isTextInvisible = label.startswith(\"inv_\") or \"$\" in label\n",
    "        mapped = label.replace(\"+complete\", \"\").replace(\"\\\\ncomplete\", \"\").replace(\"\\\\n\", \"\") \\\n",
    "            if not isInvisible and not isTextInvisible else \"\"\n",
    "        arr = gateway.new_array(gateway.jvm.int, 4)\n",
    "        arr[0], arr[1], arr[2], arr[3] = 60, 0, 60, 0\n",
    "        settings.getTransitionNames().put(t, mapped)\n",
    "        settings.getTransitionWeights().put(t, 10)\n",
    "        settings.getTransitionTimings().put(t, arr)\n",
    "        #print(t.getId(), ':', label, '--->', mapped)\n",
    "\n",
    "        settings.setNrTraces(size)\n",
    "        settings.setRandomMinInGroup(1)\n",
    "        settings.setRandomMaxInGroup(1)\n",
    "        settings.setMustReachEnd(True)\n",
    "        settings.setMustConsumeAll(False)\n",
    "        settings.setMaxTimesMarkingSeen(7)\n",
    "\n",
    "        settings.setSimulationMethod(simulator.Random) # Random, Complete, or Distinct\n",
    "\n",
    "        # settings.setRestartAfter(1000)\n",
    "        # settings.setSkipChance(0.85)\n",
    "        # settings.setAlsoConsiderPartial(False)\n",
    "    xlog = gateway.jvm.org.processmining.plugins.loggenerator.PetriNetLogGenerator.generate(\n",
    "        petri_and_marking[0], \n",
    "        petri_and_marking[1], \n",
    "        settings, \n",
    "        #None # or for output: gateway.jvm.org.processmining.plugins.kutoolbox.eventlisteners.PluginEventListenerCLI()\n",
    "        gateway.jvm.org.processmining.plugins.kutoolbox.eventlisteners.PluginEventListenerCLI()\n",
    "    )\n",
    "\n",
    "    temp_log = []\n",
    "    for trace in xlog:\n",
    "        #print([gateway.jvm.org.deckfour.xes.extension.std.XConceptExtension.instance().extractName(e) for e in trace])\n",
    "        temp_log.append([gateway.jvm.org.deckfour.xes.extension.std.XConceptExtension.instance().extractName(e) for e in trace])\n",
    "\n",
    "    model_log = remove_stuff(temp_log)\n",
    "\n",
    "\n",
    "    if check_duplicates(model_log) == True:\n",
    "        #print('Fixing duplicates')\n",
    "        model_log = fix_duplicates(model_log)\n",
    "    return(model_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_time_test(GT_log, filename, times, modellogsizes): #random model log generation\n",
    "    fitness_count = []\n",
    "    fitness_prob = []\n",
    "    precision_count = []\n",
    "    precision_prob = []\n",
    "    \n",
    "    fitness_count_errors = []\n",
    "    fitness_prob_errors = []\n",
    "    precision_count_errors = []\n",
    "    precision_prob_errors = []\n",
    "    \n",
    "    for t in range(0,len(modellogsizes)):\n",
    "        fitness_results1 = []\n",
    "        precision_results1 = []\n",
    "        fitness_results2 = []\n",
    "        precision_results2 = []\n",
    "\n",
    "        for i in range(0, times):\n",
    "            print(\"step\", modellogsizes[t], \" and \", i, \" OUT OF \", times)\n",
    "            model_log = get_model_log(filename, modellogsizes[t])\n",
    "            fitness1, fitness2 = pad_discriminator_conformance.get_distance(GT_log, model_log, \"random\")\n",
    "            precision1, precision2 = pad_discriminator_conformance.get_distance(model_log, GT_log, \"random\")\n",
    "            fitness_results1.append(fitness1)\n",
    "            fitness_results2.append(fitness2)\n",
    "            precision_results1.append(precision1)\n",
    "            precision_results2.append(precision2)\n",
    "        fitness_prob.append(statistics.median(fitness_results1))\n",
    "        fitness_count.append(statistics.median(fitness_results2))\n",
    "        precision_prob.append(statistics.median(precision_results1))\n",
    "        precision_count.append(statistics.median(precision_results2))\n",
    "        fitness_prob_errors.append(np.std(fitness_results1)/np.sqrt(10))\n",
    "        fitness_count_errors.append(np.std(fitness_results2)/np.sqrt(10))\n",
    "        precision_prob_errors.append(np.std(precision_results1)/np.sqrt(10))\n",
    "        precision_count_errors.append(np.std(precision_results2)/np.sqrt(10))\n",
    "        print(fitness_count)\n",
    "        print(fitness_prob)\n",
    "        print(fitness_count_errors)\n",
    "        print(fitness_prob_errors)\n",
    "        print(precision_count)\n",
    "        print(precision_prob)\n",
    "        print(precision_count_errors)\n",
    "        print(precision_prob_errors)\n",
    "    return(fitness_prob, fitness_count, precision_prob, precision_count, fitness_prob_errors, fitness_count_errors, precision_prob_errors, precision_count_errors)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sizes = [5, 10, 20, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "#sizes = [600, 700, 800, 900]\n",
    "\n",
    "#sizes = [5, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "\n",
    "sizes = [400, 500, 600, 700, 800, 900]\n",
    "\n",
    "#sizes = [100, 500, 1000, 1500, 2000, 2500, 3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1, fit2, prec1, prec2, errorfit1, errorfit2, errorprec1, errorprec2 = over_time_test(GT_log, \"Model11.pnml\", 10, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [5, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit1 = [0.5811467170715332, 0.5769599676132202, 0.5001965165138245, 0.49010172486305237, 0.4322972297668457, 0.18222317099571228] + fit1\n",
    "fit2 = [0.9712131596984236, 1.0, 0.17272104180945852, 0.17272104180945852, 0.17272104180945852, 0.17272104180945852]  + fit2\n",
    "errorfit1 = [0.02402501604127421, 0.013840682734899435, 0.03589896414823458, 0.055182698119664454, 0.06740481837234095, 0.06813614103424814] + errorfit1\n",
    "errorfit2 = [0.12419160697822577, 0.0879108854670524, 0.12816150510959154, 0.11988411057334236, 0.11988411057334236, 0.08008022544110256]  + errorfit2\n",
    "\n",
    "prec1 = [0.9973577260971069, 0.9962339401245117, 0.9887729287147522, 0.9982422590255737, 0.998394250869751, 0.9975321292877197] + prec1\n",
    "prec2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0] + prec2\n",
    "errorprec1 =[0.0032716706996467376, 0.0038220030894246418, 0.013988596287993212, 0.009176445085041797, 0.003450074289943527, 0.0067207795856227244]  + errorprec1\n",
    "errorprec2 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + errorprec2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fit1,fit2,errorfit1,errorfit2,prec1,prec2,errorprec1,errorprec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.errorbar(sizes, fit1, errorfit1, label=\"Recall Prob.\", ecolor = 'r', barsabove=True)\n",
    "#plt.errorbar(sizes, fit2, errorfit2, label=\"Recall Count\", color= 'g', ecolor = 'r', barsabove=True)\n",
    "plt.errorbar(sizes, prec1, errorprec1, label=\"Precision Probability\",  color= 'y', ecolor = 'r', barsabove=True)\n",
    "plt.errorbar(sizes, prec2, errorprec2, label=\"Precision Count\",  color= 'm', ecolor = 'r', barsabove=True)\n",
    "plt.xlabel('Size Model Log')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title(\"Prec. Model 11 with increasing model log size\")\n",
    "plt.savefig(\"convergence_model11_rec.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
